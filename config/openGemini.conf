[common]
  meta-join = ["{{meta_addr_1}}:8092", "{{meta_addr_2}}:8092", "{{meta_addr_3}}:8092"]
  # the shared storage-based store whether support HA.
  # write-available-first: if pt is mark offline, request will skip this pt
  # shared-storage: if pt is mark offline, request will retry until pt online
  # replication: request will retry until replication group has master
  # ha-policy = "write-available-first"
  # executor-memory-size-limit = "0"
  # executor-memory-wait-time = "0s"
  # pprof-enabled = false
  # cpu-num = 0
  # cpu-allocation-ratio = 1
  # memory-size = "0"
  # ignore-empty-tag = false
  # report-enable = true
  # node-role can be set to "reader", "writer". If no value is set, prioritize as writer, but if no reader in cluster, it is both "reader" and "writer".
  # node-role = ""
  # product-type can be left unset or set to "logkeeper".
  # product-type = ""

[meta]
  bind-address = "{{addr}}:8088"
  http-bind-address = "{{addr}}:8091"
  rpc-bind-address = "{{addr}}:8092"
  dir = "/tmp/openGemini/data/meta/{{id}}"
  #
  # expand-shards-enable = false
  # retention-autocreate = true
  # election-timeout = "1s"
  # heartbeat-timeout = "1s"
  # leader-lease-timeout = "500ms"
  # commit-timeout = "50ms"
  # cluster-tracing = true
  # logging-enabled = true
  # lease-duration = "1m0s"
  # meta-version = 0
  # split-row-threshold = 10000
  # imbalance-factor = 0.3
  # auth-enabled = false
  # https-enabled = false
  # https-certificate = ""
  # https-private-key = ""
  # ptnum-pernode = 1

  # Switch for serial balance and parallel balance
  # The default is "v1.1" of parallel balance, Serial balance is used only for setting "v1.0", Other settings use default parallel balance
  # balance-algorithm-version = "v1.1"

# [coordinator]
  # write-timeout = "10s"
  # shard-writer-timeout = "10s"
  # shard-mapper-timeout = "10s"
  # max-remote-write-connections = 100
  # max-remote-read-connections = 100
  # shard-tier = "warm"
  # rp-limit = 100
  # force-broadcast-query = false
  # time-range-limit = ["72h", "24h"]
  # tag-limit = 0

[http]
  bind-address = "{{addr}}:8086"
  flight-address = "{{addr}}:8087"
  # flight-enabled = false
  # flight-ch-factor = 2
  # flight-auth-enabled = false
  # auth-enabled = false
  # weakpwd-path = "/tmp/openGemini/weakpasswd.properties"
  # pprof-enabled = false
  # max-connection-limit = 0
  # max-concurrent-write-limit = 0
  # max-enqueued-write-limit = 0
  # enqueued-write-timeout = "30s"
  # max-concurrent-query-limit = 0
  # max-enqueued-query-limit = 0
  # enqueued-query-timeout = "5m"
  # chunk-reader-parallel = 0
  # max-body-size = 0
  # https-enabled = false
  # https-certificate = ""
  # https-private-key = ""
  # time-filter-protection = false
  # parallel-query-in-batch-enabled = true

[data]
  store-ingest-addr = "{{addr}}:8400"
  store-select-addr = "{{addr}}:8401"
  store-data-dir = "/tmp/openGemini/data"
  store-wal-dir = "/tmp/openGemini/data"
  store-meta-dir = "/tmp/openGemini/data/meta/{{id}}"
  # wal-enabled = true
  # wal-sync-interval = "100ms"
  # wal-replay-parallel = false
  # wal-replay-async = false
  # wal-replay-batch-size = "1m"
  # imm-table-max-memory-percentage = 10
  # write-cold-duration = "5s"
  # shard-mutable-size-limit = "60m"
  # node-mutable-size-limit = "200m"
  # max-write-hang-time = "15s"
  # max-concurrent-compactions = 4
  # compact-full-write-cold-duration = "1h"
  # max-full-compactions = 1
  # compact-throughput = "80m"
  # compact-throughput-burst = "90m"
  # compact-recovery = false
  # fragments-num-per-flush = 1
  # snapshot-throughput = "64m"
  # snapshot-throughput-burst = "70m"
  # Whether to cache data blocks in hot shard
  cache-table-data-block = false
  # Whether to cache meta blocks in hot shard
  cache-table-meta-block = false
  # Whether to use mmap ability
  enable-mmap-read = false
  # column-store-detached-flush-enabled = false
  # column-store-compact-enabled = false
  # If use read-meta-cache, default is 1. Equal to 0 is unused, default is 3% of memory size. 
  # enable-meta-cache = 1
  # read-meta-cache-limit-pct = 3
  # If use read-data-cache, default is 0. Equal to 0 is unused, default is 10% of memory size
  # enable-data-cache = 0
  # read-data-cache-limit-pct = 10

  # read-page-size set pageSize of read from file of datablock, default is "32kb", valid setting is "1kb"/"4kb"/"8kb"/"16kb"/"32kb"/"64kb"/"variable"
  # read-page-size = "32kb"

  # write-concurrent-limit = 0
  # open-shard-limit = 0
  # readonly = false
  # downsample-write-drop = true
  # query will be estimated abd limited by resource manager
  # max-wait-resource-time = "0s"
  # max-series-parallelism-num = 0
  # max-shards-parallelism-num = 0
  # when create group cursor, the parallelism num will be estimated by resource allocator according to the chunk-reader-threshold and min-chunk-reader-concurrency
  # chunk-reader-threshold = 0
  # min-chunk-reader-concurrency = 0
  # minimum shards number for initializing shards in parallel
  # min-shards-concurrency = 0
  # max-downsample-task-concurrency defines the max downsample task num at the same time
  # max-downsample-task-concurrency = 0
  # maximum number of series a node can hold per database. 0: unlimited
  # max-series-per-database = 0
  # manage query file handle, default enable_query_file_handle_cache is true, default max_query_cached_file_handles is cpuNum*8
  # enable_query_file_handle_cache = true
  # if max_query_cached_file_handles is 0, default query_cached_file_handles is used
  # max_query_cached_file_handles = 0

  ## Determines whether the lazy shard open is enabled.
  # lazy-load-shard-enable = true

  ## The time range for thermal shards. If the duration is set to 0s, the default value is shard group duration of the first RP.
  # thermal-shard-start-duration = "0s"
  # thermal-shard-end-duration = "0s"

  ## If queries are auto killed for store service
  # interrupt-query = true
  ## The default store mem percent threshold of start killing query
  # interrupt-sql-mem-pct = 90
  ## The default time interval of checking store mem use
  # proactive-manager-interval = "3s"

  ## Compresses temporary index files. 0 Not Compressed(default); 1 use Snappy; 2 use ZSTD
  # temporary-index-compress-mode = 0

  ## Compressing ChunkMeta in TSSP Files. 0 Not Compressed(default); 1 use Snappy; 2 use ZSTD
  # chunk-meta-compress-mode = 0

  ## Indicates whether to persist the index read cache to disk when index close
  # index-read-cache-persistent = false

# [data.ops-monitor]
  # store-http-addr = "{{addr}}:8402"
  # auth-enabled = false
  # store-https-enabled = false
  # store-https-certificate = ""

# [retention]
  # enabled = true
  # check-interval = "30m"

# [downsample]
  # enable = true
  # check-interval = "30m"

[logging]
  # format = "auto"
  # level = "info"
  path = "/tmp/openGemini/logs/{{id}}"
  # max-size = "64m"
  # max-num = 16
  # max-age = 7
  # compress-enabled = true

# [tls]
  # min-version = "TLS1.2"
  # ciphers = [
    # "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
    # "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    # "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
    # "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
  # ]

# [monitor]
  # pushers = ""
  # store-enabled = false
  # store-database = "_internal"
  # store-interval = "10s"
  # store-path = "/tmp/openGemini/metric/{{id}}/metric.data"
  # compress = false
  # https-enabled = false
  # http-endpoint = "127.0.0.1:8086"
  # username = ""
  # password = ""

[gossip]
  # enabled = true
  # log-enabled = true
  bind-address = "{{addr}}"
  store-bind-port = 8011
  meta-bind-port = 8010
  # prob-interval = '1s'
  # suspicion-mult = 4
  members = ["{{meta_addr_1}}:8010", "{{meta_addr_2}}:8010", "{{meta_addr_3}}:8010"]

# [spdy]
  # recv-window-size = 8
  # concurrent-accept-session = 4096
  # open-session-timeout = "2s"
  # session-select-timeout = "10s"
  # data-ack-timeout = "10s"
  # tcp-dial-timeout = "5s"
  # tls-enable = false
  # tls-insecure-skip-verify = false
  # tls-client-auth = false
  # tls-certificate = ""
  # tls-private-key = ""
  # tls-server-name = ""
  # conn-pool-size = 4
  # tls-client-certificate = ""
  # tls-client-private-key = ""
  # tls-ca-root = ""

# [castor]
  # enabled = false
  # pyworker-addr = ["127.0.0.1:6666"]  # format: ip:port
  # connect-pool-size = 30  # connection pool to pyworker
  # result-wait-timeout = 10  # unit: second
# [castor.detect]
  # algorithm = ['BatchDIFFERENTIATEAD','DIFFERENTIATEAD','IncrementalAD','ThresholdAD','ValueChangeAD']
  # config_filename = ['detect_base']
# [castor.fit_detect]
  # algorithm = ['BatchDIFFERENTIATEAD','DIFFERENTIATEAD','IncrementalAD','ThresholdAD','ValueChangeAD']
  # config_filename = ['detect_base']

# [sherlock]
  # sherlock-enable = false
  # collect-interval = "10s"
  # cpu-max-limit = 95
  # dump-path = "/tmp"
  # max-num = 32
  # max-age = 7
# [sherlock.cpu]
  # enable = false
  # min = 30
  # diff = 25
  # abs = 70
  # cool-down = "10m"
# [sherlock.memory]
  # enable = false
  # min = 25
  # diff = 25
  # abs = 80
  # cool-down = "10m"
# [sherlock.goroutine]
  # enable = false
  # min = 10000
  # diff = 20
  # abs = 20000
  # max = 100000
  # cool-down = "30m"

#[clv_config]
  # enabled = false
  # q-max is maximum token length of V-token(Variable Length Token) tokenizer.
  # q-max = 7
  # document-count indicates how many documents are collected for generating V-token tokenizer.
  # document-count = 500000
  # token-threshold indicates the pruning frequency of all tokens for the collected documents.
  # token-threshold = 100


[io-detector]
  # paths = []

[spec-limit]
  enable-query-when-exceed = true
  query-series-limit = 0
  query-schema-limit = 0

[subscriber]
  # enabled = false
  # http-timeout = "30s"
  # insecure-skip-verify = false
  # https-certificate = ""
  # write-buffer-size = 100
  # write-concurrency = 15

###
### [continuous_queries]
###
### Controls how continuous queries are run within openGemini.
###

[continuous_queries]
  ## Determines whether the continuous queries service is enabled.
  # enabled = true
  ## The interval for how often continuous queries will be checked if they need to run.
  # run-interval = "1s"
  ## concurrent exec continues queries goroutines number. Default 1/3 of cpu number, at least 1 and at most 5.
  # max-process-CQ-number = 0

[hierarchical_storage]
  ## If this flag is set to false, close  hierarchical storage service
  # enabled = false
  ## Run interval time for checking hierarchical storage.
  # run-interval= "1m"
  ## max process number for shard moving
  # max-process-HS-number =1